{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RF Breast Cancer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khoing0810/RF_breast_cancer/blob/master/RF_Breast_Cancer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W67AtWPplOMo",
        "colab_type": "text"
      },
      "source": [
        "# **Official Codes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4UNtiihk_x4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score #Classfication\n",
        "\n",
        "\n",
        "column = ['Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape', 'Marginal Adhesion', 'Single Epithelial Cell Size', \n",
        "          'Bare Nuclei', 'Normal Nucleoli', 'Mitoses', 'Result'] # Sources: https://github.com/jbrownlee/Datasets/blob/master/breast-cancer-wisconsin.names\n",
        "dataset = pd.read_csv(\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/breast-cancer-wisconsin.csv\", names=column)\n",
        "print(dataset.shape)\n",
        "dataset = dataset.replace('?', np.NaN) # [IMPORTANT]\n",
        "dataset.dropna(inplace=True) # [IMPORTANT]\n",
        "array = dataset.values\n",
        "print(dataset.shape)\n",
        "print(dataset.describe())\n",
        "\n",
        "\n",
        "X_variables_split = array[:, 0:8]\n",
        "Y_variables_split = array[:, 8]\n",
        "\n",
        "#Training\n",
        "Y_variables_split = pd.cut(Y_variables_split,2,labels=['Benign', 'Malignant']) \n",
        "\"\"\"[IMPORTANT] Since we only have 2 outcomes, 2 and 4, we can split into 2 equal bins and \n",
        "categorize these bins as Benign (2) and Malign (4) respectively\"\"\"\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_variables_split, Y_variables_split, test_size=0.2, random_state=None) \n",
        "\"\"\"Set random_state to None so it can be different each time and graph through each trial\"\"\"\n",
        "classifier = RandomForestClassifier(n_estimators=20, random_state=0).fit(X_train, Y_train)\n",
        "\n",
        "cross_validation = cross_val_score(classifier,X_variables_split,Y_variables_split,scoring='accuracy',cv=10)\n",
        "print('Accuracy scoring: %f' % (cross_validation.mean()))\n",
        "prediction = classifier.predict(X_test) #This predict the results based from the X_train, using X_test as a testing dataset\n",
        "\n",
        "#Classfication\n",
        "print(confusion_matrix(Y_test,prediction)) # [IMPORTANT]\n",
        "print(classification_report(Y_test,prediction)) # [IMPORTANT]\n",
        "print(accuracy_score(Y_test, prediction)) # [IMPORTANT]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4_n3sp24tim",
        "colab_type": "text"
      },
      "source": [
        "# Pre-Final"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wjx6fyzK8Zrb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Pre-Final\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score #Classfication\n",
        "\n",
        "\n",
        "column = ['Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape', 'Marginal Adhesion', 'Single Epithelial Cell Size', 'Bare Nuclei', 'Normal Nucleoli', 'Mitoses', 'Result'] #Cite sources\n",
        "dataset = pd.read_csv(\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/breast-cancer-wisconsin.csv\", names=column)\n",
        "print(dataset.shape)\n",
        "dataset = dataset.replace('?', np.NaN) # [IMPORTANT]\n",
        "dataset.dropna(inplace=True) # [IMPORTANT]\n",
        "array = dataset.values\n",
        "print(dataset.shape)\n",
        "print(dataset.describe())\n",
        "\n",
        "\n",
        "X_variables_split = array[:, 0:8]\n",
        "Y_variables_split = array[:, 8]\n",
        "\n",
        "\"\"\"for grp in dataset:\n",
        "  for i in grp:\n",
        "    if i == \"?\":\n",
        "      \"\"\" #This doesn't work so I just replace all \"?\" to NaN values and use .dropna to drop all the rows that \n",
        "      #contain the according NaN values.\n",
        "#Training Timeeeeeee\n",
        "Y_variables_split = pd.cut(Y_variables_split,2,labels=['Benign', 'Malignant']) #[IMPORTANT] Since we only have 2 outcomes, 2 and 4, we can split into 2 equal bins and categorize these bins as Benign (2) and Malign (4) respectively\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_variables_split, Y_variables_split, test_size=0.2, random_state=None) #Set random_state to None so it can be different each time and graph through each trial\n",
        "classifier = RandomForestClassifier(n_estimators=20, random_state=0).fit(X_train, Y_train) #if have time, try to find the value of n_estimators such that RMSE is a minimum // Also, changed from RandomForestRegressor to RandomForestClassifier\n",
        "\"\"\"X_array = np.array(X_train)\n",
        "Y_array = np.array(Y_train)\n",
        "X_array = bin(X_array)\n",
        "Y_array = bin(Y_array)\"\"\"\n",
        "#fitting = classifier.fit(X_train, Y_train) | Already put in the classifier variable\n",
        "cross_validation = cross_val_score(classifier,X_variables_split,Y_variables_split,scoring='accuracy',cv=10)\n",
        "print('Accuracy scoring: %f' % (cross_validation.mean()))\n",
        "prediction = classifier.predict(X_test) #This predict the results based from the X_train, using X_test as a testing dataset\n",
        "\n",
        "#Classfication Time\n",
        "\"\"\"float_Y_test = []\n",
        "for i in Y_test:\n",
        "  float_Y_test.append(float(i))\n",
        "print(Y_test, Y_pred) // NOT NEEDED ANYMORE AFTER REPLACING CLASSIFIER\"\"\"\n",
        "print(confusion_matrix(Y_test,prediction)) # [IMPORTANT]\n",
        "print(classification_report(Y_test,prediction)) # [IMPORTANT]\n",
        "print(accuracy_score(Y_test, prediction)) # [IMPORTANT]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUI3py0G4nVf",
        "colab_type": "text"
      },
      "source": [
        "# Source Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U95Me8bII0eK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Source: https://machinelearningmastery.com/machine-learning-in-python-step-by-step/ This is for the very first tutorial to understand basics of machine learning\n",
        "#This paves a foundation for the final stage of the project\n",
        "from pandas import read_csv\n",
        "from pandas.plotting import scatter_matrix\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# All the lines below until line 37 use 'from pandas import read_csv' execution\n",
        "# Load dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv\"\n",
        "names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\n",
        "# this lets python read the csv file from the 'url' with the data dimensions name of\n",
        "# the list imported\n",
        "dataset = read_csv(url, names=names)\n",
        "\n",
        "# Dimensions of Datasets - shape\n",
        "# Print out the shape (which has 150 instances and 5 attributes (which imported from the list)\n",
        "print(dataset.shape)\n",
        "\n",
        "# Sneakpeek at data\n",
        "print(dataset.head(20))  # Print first 20 rows from the dataset\n",
        "\n",
        "# Descriptions - Statistical summary\n",
        "print(dataset.describe())\n",
        "\n",
        "# class distribution\n",
        "print(dataset.groupby('class').size())  # this will group the dataset by class and return its class size/amount\n",
        "\n",
        "## Let's move to data visualization - this uses matplotlib import pyplot\n",
        "\n",
        "# Univariate plots - to better understand each attribute/category\n",
        "# box and whisker plot\n",
        "dataset.plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False)\n",
        "pyplot.show()\n",
        "\n",
        "# histograms\n",
        "dataset.hist()\n",
        "pyplot.show()\n",
        "\n",
        "# scatter plot matrix - multivariate plot - to understand relationship between attributes\n",
        "scatter_matrix(dataset)\n",
        "pyplot.show()\n",
        "\n",
        "## Evaluate algorithms\n",
        "# Create a validation dataset - to know whether created model is good\n",
        "# Split-out validation dataset\n",
        "array = dataset.values\n",
        "X = array[:, 0:4]\n",
        "Y = array[:, 4]\n",
        "\n",
        "X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=0.20, random_state=1)\n",
        "# You now have training data in X & Y train for preparing models and X & Y validation sets that we can use later\n",
        "\n",
        "# Test Harness (use stratified 10-fold / k-fold cross validation to estimate model accuracy)\n",
        "# This will split dataset into 10 parts, train on 9 and test on 1 and repeat for all combinations.\n",
        "# Stratified means that each fold or split aims to have SAME DISTRIBUTION OF EXAMPLE by class as exist in whole training dataset\n",
        "# Set the random seed via random_state to a fixed number to ensure each algorithm is evaluated on the same splits of training dataset\n",
        "# Use metric of 'accuracy' to evaluate models\n",
        "\n",
        "# Build models\n",
        "# We don't know which algorithms would be good on this problem\n",
        "# So we test 6 different algorithms\n",
        "\n",
        "# Spot Check Algorithms\n",
        "# Don't do empty list then .append() bc it would give u errors (unless u are in python 3.7)\n",
        "models = [('LR', LogisticRegression(solver='liblinear', multi_class='ovr')), ('LDA', LinearDiscriminantAnalysis()),\n",
        "          ('KNN', KNeighborsClassifier()), ('CART', DecisionTreeClassifier()), ('NB', GaussianNB()),\n",
        "          ('SVM', SVC(gamma='auto'))]\n",
        "# evaluate each model in turn\n",
        "results = []\n",
        "names = []\n",
        "for name, model in models:\n",
        "    kfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n",
        "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\n",
        "    results.append(cv_results)\n",
        "    names.append(name)\n",
        "    print('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))\n",
        "\n",
        "# Compare algorithms\n",
        "\"\"\"pyplot.boxplot(results, labels=names)\n",
        "pyplot.title('Algorithm Comparison')\n",
        "pyplot.show()\"\"\"\n",
        "\n",
        "# Make predictions on validation dataset\n",
        "model = SVC(gamma='auto')  # What is gamma?\n",
        "model.fit(X_train, Y_train)\n",
        "predictions = model.predict(X_validation)\n",
        "\n",
        "# evaluate predictions\n",
        "print(accuracy_score(Y_validation, predictions))\n",
        "print(confusion_matrix(Y_validation, predictions))\n",
        "print(classification_report(Y_validation, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIkRJG1G4wcp",
        "colab_type": "text"
      },
      "source": [
        "# Regression Version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9M6MvjiWC1M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Regression Version of My Project\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "name = ['Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape', 'Marginal Adhesion', 'Single Epithelial Cell Size', 'Bare Nuclei', 'Normal Nucleoli', 'Mitoses','Class']\n",
        "dataset = pd.read_csv(\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/breast-cancer-wisconsin.csv\", names=name)\n",
        "print(dataset.shape)\n",
        "dataset = dataset.replace('?', np.NaN)\n",
        "dataset.dropna(inplace=True)\n",
        "array = dataset.values\n",
        "print(dataset.shape)\n",
        "print(dataset.describe())\n",
        "\n",
        "X_variables_split = array[:, 0:8]\n",
        "Y_variables_split = array[:, 8]\n",
        "\n",
        "#Training Timeeeeeee\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_variables_split, Y_variables_split, test_size=0.2, random_state=0)\n",
        "regressor = RandomForestRegressor(n_estimators=100, random_state=0) #if have time, try to find the value of n_estimators such that RMSE is a minimum // Also, changed from RandomForestRegressor to RandomForestClassifier\n",
        "regressor.fit(X_train, Y_train)\n",
        "Y_pred = regressor.predict(X_test)\n",
        "\n",
        "print('Mean Absolute Error:', metrics.mean_absolute_error(Y_test, Y_pred))\n",
        "print('Mean Squared Error:', metrics.mean_squared_error(Y_test, Y_pred))\n",
        "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, Y_pred)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}